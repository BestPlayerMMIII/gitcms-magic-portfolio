{
  "id": "sholo",
  "schemaId": "project",
  "data": {
    "header": {
      "title": "SHolo",
      "excerpt": "Control a 3D object in real-time using only hand gestures and eye movements through a standard webcam — exploring computer vision and gesture tracking."
    },
    "technologies": [],
    "githubUrl": "",
    "liveUrl": "",
    "overview": "<p style=\"text-align: center;\">Transform your webcam and monitor into a desktop <strong>hologram</strong>!</p><p style=\"text-align: center;\">SHolo maps eye position to a virtual space and uses hand gestures to impart angular velocity or stop motion, producing a Tony Stark–style floating-object interaction on a normal screen.</p>",
    "content": "<h2>The Vision</h2><p>There’s a scene I’ve always loved in <strong>Iron-Man 2</strong>: Tony Stark standing in front of floating holograms, manipulating them with a casual swipe of his hand.<br>It’s cinematic, but it’s also the perfect metaphor for <strong>human-computer interaction</strong>: intuitive, responsive, alive.</p><p>I wanted to recreate that feeling on a regular desktop. <strong>No special hardware</strong>, just a monitor and a webcam.<br>And that’s how <strong>SHolo (Screen Hologram)</strong> was born.</p><p>SHolo transforms your screen into a 3D holographic illusion: it tracks your eyes to create real-time parallax and lets you move objects through natural hand gestures.<br>Look around, swipe, or hold your hand still: the object responds like it’s truly suspended in front of you!</p><h2>The Concept</h2><p>The idea came from a simple observation:<br>when you move your head, the perspective of an object shifts: that’s what makes 3D feel real.</p><p>If you could <strong>map that head or eye movement</strong> to a virtual camera in 3D space, the illusion would follow you naturally.<br>So I started experimenting with computer vision, tracking both the <strong>eyes</strong> and <strong>hands</strong> through a single webcam feed.</p><ul><li><p>Eye movement defines the <strong>camera position</strong>, creating realistic depth through parallax.</p></li><li><p>Hand gestures define <strong>motion</strong>: a lateral swipe adds angular velocity, and an open palm stops the spin.</p></li></ul><p>That combination of parallax and inertia turned a flat monitor into something that <em>feels</em> like a holographic projection.</p><h1>More Technical Now…</h1><p>At its core, SHolo is an experimental <strong>Python application</strong> that brings together <strong>MediaPipe</strong>, <strong>OpenCV</strong>, <strong>Trimesh</strong>, and <strong>PyVista/Pyglet</strong> into a multi-threaded 3D environment.</p><h2>Core pipeline</h2><ol><li><p><strong>Face &amp; eye tracking</strong> → MediaPipe Face Mesh locates eyes, estimates inter-pupillary distance (IPD), and maps gaze offset to yaw/pitch camera rotation.</p></li><li><p><strong>Hand tracking</strong> → MediaPipe Hands detects swipes and open-palm gestures to control motion.</p></li><li><p><strong>3D rendering</strong> → The scene is rendered in Trimesh via PyVista/Pyglet, with live transformations driven by the detected inputs.</p></li><li><p><strong>Thread orchestration</strong> → The OpenCV detection loop runs independently from the Pyglet viewer to maintain responsiveness and avoid frame drops.</p></li></ol><p>The result: a seamless interaction where your eyes control <em>view</em>, and your hands control <em>motion</em>.</p><hr><h3>Tech Stack</h3><table style=\"min-width: 50px;\"><colgroup><col style=\"min-width: 25px;\"><col style=\"min-width: 25px;\"></colgroup><tbody><tr><th colspan=\"1\" rowspan=\"1\"><p>Purpose</p></th><th colspan=\"1\" rowspan=\"1\"><p>Tool</p></th></tr><tr><td colspan=\"1\" rowspan=\"1\"><p>Language</p></td><td colspan=\"1\" rowspan=\"1\"><p>Python 3.x</p></td></tr><tr><td colspan=\"1\" rowspan=\"1\"><p>Computer Vision</p></td><td colspan=\"1\" rowspan=\"1\"><p>OpenCV</p></td></tr><tr><td colspan=\"1\" rowspan=\"1\"><p>Landmark Detection</p></td><td colspan=\"1\" rowspan=\"1\"><p>MediaPipe (Face Mesh + Hands)</p></td></tr><tr><td colspan=\"1\" rowspan=\"1\"><p>3D Geometry</p></td><td colspan=\"1\" rowspan=\"1\"><p>Trimesh</p></td></tr><tr><td colspan=\"1\" rowspan=\"1\"><p>Rendering</p></td><td colspan=\"1\" rowspan=\"1\"><p>PyVista / Pyglet</p></td></tr><tr><td colspan=\"1\" rowspan=\"1\"><p>Numerics</p></td><td colspan=\"1\" rowspan=\"1\"><p>NumPy</p></td></tr></tbody></table><p>Key modules include <code>eyes_tracking.py</code>, <code>hands_tracking.py</code>, and <code>view_3d.py</code>, orchestrated by <code>SHolo.py</code>.<br>Everything is modular, threaded, and tunable via <code>config.py</code>.</p><hr><h3>🧩 Key Features</h3><ul><li><p>👁️ <strong>Eye-driven parallax</strong> — gaze position translates into real-time camera movement for a holographic depth illusion.</p></li><li><p>✋ <strong>Gesture control</strong> — lateral hand swipes add inertia-based spin; open palm instantly halts it.</p></li><li><p>💨 <strong>Inertia simulation</strong> — smooth angular velocity and deceleration curves make motion feel physically real.</p></li><li><p>🪶 <strong>Threaded runtime</strong> — tracking and rendering run concurrently for fluid, lag-free motion.</p></li><li><p>🧾 <strong>Debug overlay</strong> — OpenCV window visualizes landmarks and gesture cues for easy tuning.</p></li></ul><hr><h3>🔬 Challenges &amp; Learning</h3><p>Developing SHolo required bridging two normally separate worlds — <strong>computer vision</strong> and <strong>3D rendering</strong>.</p><p>I experimented extensively with <strong>mathematical models</strong> to make the illusion convincing:</p><ul><li><p>Mapping 2D landmarks into 3D camera transforms required careful tuning of IPD-based depth estimates and small-angle approximations.</p></li><li><p>Gesture recognition had to feel natural, not twitchy — so I implemented cooldowns, thresholds, and easing curves to simulate momentum.</p></li><li><p>Running both detection and rendering in parallel threads demanded precision and attention to timing details.</p></li></ul><p>Every line of code became an exercise in balancing <strong>accuracy</strong>, <strong>realism</strong>, and <strong>responsiveness</strong> — a dance between art and engineering.</p><hr><h3>💭 Why It Matters</h3><p><strong>SHolo</strong> started as a technical curiosity, but became a meditation on <em>interaction itself</em>.<br>It shows that immersive interfaces don’t always need complex hardware —<br>sometimes, all it takes is a clever use of data and imagination.</p><p>It’s a small step toward a future where digital and physical spaces feel connected —<br>where technology responds not to clicks, but to presence, motion, and intent.</p><hr><h3>🔮 Future Plans</h3><ul><li><p>Add calibration for camera distance and IPD for even greater realism.</p></li><li><p>Smooth input signals with temporal filters (EMA/Kalman) to reduce jitter.</p></li><li><p>Expand gestures to include pinch, translate, and multi-object control.</p></li><li><p>Build a lightweight <strong>WebGL</strong> version for easy sharing and experimentation.</p></li></ul><hr><h3>🧩 My Role</h3><ul><li><p>Designed, implemented, and integrated every subsystem — from vision tracking to scene rendering.</p></li><li><p>Tuned the motion model, thresholds, and filters by hand to achieve a natural holographic effect.</p></li><li><p>Documented and open-sourced the project as a learning experiment for creative human–computer interaction.</p></li></ul><hr><h3>🧠 What I Learned</h3><p>Building SHolo taught me that <strong>innovation often starts with curiosity</strong> — not with a goal, but with a question:</p><blockquote><p>“What if I could make my computer <em>feel</em> alive?”</p></blockquote><p>It made me think like both a scientist and an artist: analytical when mapping gaze coordinates, creative when designing how motion should <em>feel</em>.<br>And it reminded me that even small experiments can stretch what’s possible. That courage and curiosity, together, can turn a simple webcam into a hologram.</p>",
    "metadata": {}
  },
  "metadata": {
    "id": "sholo",
    "createdAt": "2025-10-30T17:46:48.681Z",
    "updatedAt": "2025-10-30T17:46:48.681Z",
    "status": "draft"
  }
}